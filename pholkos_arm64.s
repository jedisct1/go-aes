//go:build !purego

#include "textflag.h"

// Pholkos-256 encryption using ARM Crypto extensions
// func pholkos256EncryptAsm(block *Pholkos256Block, rtk *[17][2]Block)
TEXT ·pholkos256EncryptAsm(SB),NOSPLIT,$0
	MOVD block+0(FP), R0
	MOVD rtk+8(FP), R1

	// Load state blocks
	VLD1 (R0), [V0.B16, V1.B16]

	// Create zero register for AESE
	VEOR V15.B16, V15.B16, V15.B16

	// Initial round tweakey addition (RTK[0])
	VLD1.P 32(R1), [V2.B16, V3.B16]
	VEOR V2.B16, V0.B16, V0.B16
	VEOR V3.B16, V1.B16, V1.B16

	// Step 0: rounds 1-2
	// Round 1
	VLD1.P 32(R1), [V2.B16, V3.B16]
	AESE V15.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V2.B16, V0.B16, V0.B16
	AESE V15.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V3.B16, V1.B16, V1.B16
	// Round 2
	VLD1.P 32(R1), [V2.B16, V3.B16]
	AESE V15.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V2.B16, V0.B16, V0.B16
	AESE V15.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V3.B16, V1.B16, V1.B16
	// Word permutation π256 (swap odd 32-bit words)
	VMOV V0.S[1], V4.S[0]
	VMOV V0.S[3], V4.S[1]
	VMOV V1.S[1], V0.S[1]
	VMOV V1.S[3], V0.S[3]
	VMOV V4.S[0], V1.S[1]
	VMOV V4.S[1], V1.S[3]

	// Step 1: rounds 3-4
	VLD1.P 32(R1), [V2.B16, V3.B16]
	AESE V15.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V2.B16, V0.B16, V0.B16
	AESE V15.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V3.B16, V1.B16, V1.B16
	VLD1.P 32(R1), [V2.B16, V3.B16]
	AESE V15.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V2.B16, V0.B16, V0.B16
	AESE V15.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V3.B16, V1.B16, V1.B16
	VMOV V0.S[1], V4.S[0]
	VMOV V0.S[3], V4.S[1]
	VMOV V1.S[1], V0.S[1]
	VMOV V1.S[3], V0.S[3]
	VMOV V4.S[0], V1.S[1]
	VMOV V4.S[1], V1.S[3]

	// Step 2: rounds 5-6
	VLD1.P 32(R1), [V2.B16, V3.B16]
	AESE V15.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V2.B16, V0.B16, V0.B16
	AESE V15.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V3.B16, V1.B16, V1.B16
	VLD1.P 32(R1), [V2.B16, V3.B16]
	AESE V15.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V2.B16, V0.B16, V0.B16
	AESE V15.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V3.B16, V1.B16, V1.B16
	VMOV V0.S[1], V4.S[0]
	VMOV V0.S[3], V4.S[1]
	VMOV V1.S[1], V0.S[1]
	VMOV V1.S[3], V0.S[3]
	VMOV V4.S[0], V1.S[1]
	VMOV V4.S[1], V1.S[3]

	// Step 3: rounds 7-8
	VLD1.P 32(R1), [V2.B16, V3.B16]
	AESE V15.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V2.B16, V0.B16, V0.B16
	AESE V15.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V3.B16, V1.B16, V1.B16
	VLD1.P 32(R1), [V2.B16, V3.B16]
	AESE V15.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V2.B16, V0.B16, V0.B16
	AESE V15.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V3.B16, V1.B16, V1.B16
	VMOV V0.S[1], V4.S[0]
	VMOV V0.S[3], V4.S[1]
	VMOV V1.S[1], V0.S[1]
	VMOV V1.S[3], V0.S[3]
	VMOV V4.S[0], V1.S[1]
	VMOV V4.S[1], V1.S[3]

	// Step 4: rounds 9-10
	VLD1.P 32(R1), [V2.B16, V3.B16]
	AESE V15.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V2.B16, V0.B16, V0.B16
	AESE V15.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V3.B16, V1.B16, V1.B16
	VLD1.P 32(R1), [V2.B16, V3.B16]
	AESE V15.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V2.B16, V0.B16, V0.B16
	AESE V15.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V3.B16, V1.B16, V1.B16
	VMOV V0.S[1], V4.S[0]
	VMOV V0.S[3], V4.S[1]
	VMOV V1.S[1], V0.S[1]
	VMOV V1.S[3], V0.S[3]
	VMOV V4.S[0], V1.S[1]
	VMOV V4.S[1], V1.S[3]

	// Step 5: rounds 11-12
	VLD1.P 32(R1), [V2.B16, V3.B16]
	AESE V15.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V2.B16, V0.B16, V0.B16
	AESE V15.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V3.B16, V1.B16, V1.B16
	VLD1.P 32(R1), [V2.B16, V3.B16]
	AESE V15.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V2.B16, V0.B16, V0.B16
	AESE V15.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V3.B16, V1.B16, V1.B16
	VMOV V0.S[1], V4.S[0]
	VMOV V0.S[3], V4.S[1]
	VMOV V1.S[1], V0.S[1]
	VMOV V1.S[3], V0.S[3]
	VMOV V4.S[0], V1.S[1]
	VMOV V4.S[1], V1.S[3]

	// Step 6: rounds 13-14
	VLD1.P 32(R1), [V2.B16, V3.B16]
	AESE V15.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V2.B16, V0.B16, V0.B16
	AESE V15.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V3.B16, V1.B16, V1.B16
	VLD1.P 32(R1), [V2.B16, V3.B16]
	AESE V15.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V2.B16, V0.B16, V0.B16
	AESE V15.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V3.B16, V1.B16, V1.B16
	VMOV V0.S[1], V4.S[0]
	VMOV V0.S[3], V4.S[1]
	VMOV V1.S[1], V0.S[1]
	VMOV V1.S[3], V0.S[3]
	VMOV V4.S[0], V1.S[1]
	VMOV V4.S[1], V1.S[3]

	// Step 7 (final): rounds 15-16, no permutation after
	// Round 15
	VLD1.P 32(R1), [V2.B16, V3.B16]
	AESE V15.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V2.B16, V0.B16, V0.B16
	AESE V15.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V3.B16, V1.B16, V1.B16
	// Round 16 (final): no AESMC
	VLD1 (R1), [V2.B16, V3.B16]
	AESE V15.B16, V0.B16
	VEOR V2.B16, V0.B16, V0.B16
	AESE V15.B16, V1.B16
	VEOR V3.B16, V1.B16, V1.B16

	// Store result
	MOVD block+0(FP), R0
	VST1 [V0.B16, V1.B16], (R0)
	RET


// Pholkos-256 decryption using ARM Crypto extensions
// func pholkos256DecryptAsm(block *Pholkos256Block, rtk *[17][2]Block)
TEXT ·pholkos256DecryptAsm(SB),NOSPLIT,$0
	MOVD block+0(FP), R0
	MOVD rtk+8(FP), R1

	// Load state blocks
	VLD1 (R0), [V0.B16, V1.B16]

	// Zero register for AESD operations
	VEOR V15.B16, V15.B16, V15.B16

	// Go to RTK[16] (offset = 16 * 32 = 512)
	ADD $512, R1, R3

	// Step 7 (first in reverse): undo rounds 16, 15
	// Round 16 final: XOR key, then InvSR+InvSB
	VLD1 (R3), [V2.B16, V3.B16]
	VEOR V2.B16, V0.B16, V0.B16
	VEOR V3.B16, V1.B16, V1.B16
	AESD V15.B16, V0.B16
	AESD V15.B16, V1.B16

	// Round 15: XOR key, InvMC, InvSR+InvSB
	SUB $32, R3, R3
	VLD1 (R3), [V2.B16, V3.B16]
	VEOR V2.B16, V0.B16, V0.B16
	VEOR V3.B16, V1.B16, V1.B16
	AESIMC V0.B16, V0.B16
	AESIMC V1.B16, V1.B16
	AESD V15.B16, V0.B16
	AESD V15.B16, V1.B16

	// Step 6 inverse: undo permutation, then rounds 14, 13
	VMOV V0.S[1], V4.S[0]
	VMOV V0.S[3], V4.S[1]
	VMOV V1.S[1], V0.S[1]
	VMOV V1.S[3], V0.S[3]
	VMOV V4.S[0], V1.S[1]
	VMOV V4.S[1], V1.S[3]
	// Round 14
	SUB $32, R3, R3
	VLD1 (R3), [V2.B16, V3.B16]
	VEOR V2.B16, V0.B16, V0.B16
	VEOR V3.B16, V1.B16, V1.B16
	AESIMC V0.B16, V0.B16
	AESIMC V1.B16, V1.B16
	AESD V15.B16, V0.B16
	AESD V15.B16, V1.B16
	// Round 13
	SUB $32, R3, R3
	VLD1 (R3), [V2.B16, V3.B16]
	VEOR V2.B16, V0.B16, V0.B16
	VEOR V3.B16, V1.B16, V1.B16
	AESIMC V0.B16, V0.B16
	AESIMC V1.B16, V1.B16
	AESD V15.B16, V0.B16
	AESD V15.B16, V1.B16

	// Step 5 inverse
	VMOV V0.S[1], V4.S[0]
	VMOV V0.S[3], V4.S[1]
	VMOV V1.S[1], V0.S[1]
	VMOV V1.S[3], V0.S[3]
	VMOV V4.S[0], V1.S[1]
	VMOV V4.S[1], V1.S[3]
	SUB $32, R3, R3
	VLD1 (R3), [V2.B16, V3.B16]
	VEOR V2.B16, V0.B16, V0.B16
	VEOR V3.B16, V1.B16, V1.B16
	AESIMC V0.B16, V0.B16
	AESIMC V1.B16, V1.B16
	AESD V15.B16, V0.B16
	AESD V15.B16, V1.B16
	SUB $32, R3, R3
	VLD1 (R3), [V2.B16, V3.B16]
	VEOR V2.B16, V0.B16, V0.B16
	VEOR V3.B16, V1.B16, V1.B16
	AESIMC V0.B16, V0.B16
	AESIMC V1.B16, V1.B16
	AESD V15.B16, V0.B16
	AESD V15.B16, V1.B16

	// Step 4 inverse
	VMOV V0.S[1], V4.S[0]
	VMOV V0.S[3], V4.S[1]
	VMOV V1.S[1], V0.S[1]
	VMOV V1.S[3], V0.S[3]
	VMOV V4.S[0], V1.S[1]
	VMOV V4.S[1], V1.S[3]
	SUB $32, R3, R3
	VLD1 (R3), [V2.B16, V3.B16]
	VEOR V2.B16, V0.B16, V0.B16
	VEOR V3.B16, V1.B16, V1.B16
	AESIMC V0.B16, V0.B16
	AESIMC V1.B16, V1.B16
	AESD V15.B16, V0.B16
	AESD V15.B16, V1.B16
	SUB $32, R3, R3
	VLD1 (R3), [V2.B16, V3.B16]
	VEOR V2.B16, V0.B16, V0.B16
	VEOR V3.B16, V1.B16, V1.B16
	AESIMC V0.B16, V0.B16
	AESIMC V1.B16, V1.B16
	AESD V15.B16, V0.B16
	AESD V15.B16, V1.B16

	// Step 3 inverse
	VMOV V0.S[1], V4.S[0]
	VMOV V0.S[3], V4.S[1]
	VMOV V1.S[1], V0.S[1]
	VMOV V1.S[3], V0.S[3]
	VMOV V4.S[0], V1.S[1]
	VMOV V4.S[1], V1.S[3]
	SUB $32, R3, R3
	VLD1 (R3), [V2.B16, V3.B16]
	VEOR V2.B16, V0.B16, V0.B16
	VEOR V3.B16, V1.B16, V1.B16
	AESIMC V0.B16, V0.B16
	AESIMC V1.B16, V1.B16
	AESD V15.B16, V0.B16
	AESD V15.B16, V1.B16
	SUB $32, R3, R3
	VLD1 (R3), [V2.B16, V3.B16]
	VEOR V2.B16, V0.B16, V0.B16
	VEOR V3.B16, V1.B16, V1.B16
	AESIMC V0.B16, V0.B16
	AESIMC V1.B16, V1.B16
	AESD V15.B16, V0.B16
	AESD V15.B16, V1.B16

	// Step 2 inverse
	VMOV V0.S[1], V4.S[0]
	VMOV V0.S[3], V4.S[1]
	VMOV V1.S[1], V0.S[1]
	VMOV V1.S[3], V0.S[3]
	VMOV V4.S[0], V1.S[1]
	VMOV V4.S[1], V1.S[3]
	SUB $32, R3, R3
	VLD1 (R3), [V2.B16, V3.B16]
	VEOR V2.B16, V0.B16, V0.B16
	VEOR V3.B16, V1.B16, V1.B16
	AESIMC V0.B16, V0.B16
	AESIMC V1.B16, V1.B16
	AESD V15.B16, V0.B16
	AESD V15.B16, V1.B16
	SUB $32, R3, R3
	VLD1 (R3), [V2.B16, V3.B16]
	VEOR V2.B16, V0.B16, V0.B16
	VEOR V3.B16, V1.B16, V1.B16
	AESIMC V0.B16, V0.B16
	AESIMC V1.B16, V1.B16
	AESD V15.B16, V0.B16
	AESD V15.B16, V1.B16

	// Step 1 inverse
	VMOV V0.S[1], V4.S[0]
	VMOV V0.S[3], V4.S[1]
	VMOV V1.S[1], V0.S[1]
	VMOV V1.S[3], V0.S[3]
	VMOV V4.S[0], V1.S[1]
	VMOV V4.S[1], V1.S[3]
	SUB $32, R3, R3
	VLD1 (R3), [V2.B16, V3.B16]
	VEOR V2.B16, V0.B16, V0.B16
	VEOR V3.B16, V1.B16, V1.B16
	AESIMC V0.B16, V0.B16
	AESIMC V1.B16, V1.B16
	AESD V15.B16, V0.B16
	AESD V15.B16, V1.B16
	SUB $32, R3, R3
	VLD1 (R3), [V2.B16, V3.B16]
	VEOR V2.B16, V0.B16, V0.B16
	VEOR V3.B16, V1.B16, V1.B16
	AESIMC V0.B16, V0.B16
	AESIMC V1.B16, V1.B16
	AESD V15.B16, V0.B16
	AESD V15.B16, V1.B16

	// Step 0 inverse (with permutation to undo)
	VMOV V0.S[1], V4.S[0]
	VMOV V0.S[3], V4.S[1]
	VMOV V1.S[1], V0.S[1]
	VMOV V1.S[3], V0.S[3]
	VMOV V4.S[0], V1.S[1]
	VMOV V4.S[1], V1.S[3]
	SUB $32, R3, R3
	VLD1 (R3), [V2.B16, V3.B16]
	VEOR V2.B16, V0.B16, V0.B16
	VEOR V3.B16, V1.B16, V1.B16
	AESIMC V0.B16, V0.B16
	AESIMC V1.B16, V1.B16
	AESD V15.B16, V0.B16
	AESD V15.B16, V1.B16
	SUB $32, R3, R3
	VLD1 (R3), [V2.B16, V3.B16]
	VEOR V2.B16, V0.B16, V0.B16
	VEOR V3.B16, V1.B16, V1.B16
	AESIMC V0.B16, V0.B16
	AESIMC V1.B16, V1.B16
	AESD V15.B16, V0.B16
	AESD V15.B16, V1.B16

	// Initial round key (RTK[0]) - just XOR
	MOVD rtk+8(FP), R1
	VLD1 (R1), [V2.B16, V3.B16]
	VEOR V2.B16, V0.B16, V0.B16
	VEOR V3.B16, V1.B16, V1.B16

	// Store result
	MOVD block+0(FP), R0
	VST1 [V0.B16, V1.B16], (R0)
	RET


// Pholkos-512 encryption using ARM Crypto extensions
// func pholkos512EncryptAsm(block *Pholkos512Block, rtk *[21][4]Block)
TEXT ·pholkos512EncryptAsm(SB),NOSPLIT,$0
	MOVD block+0(FP), R0
	MOVD rtk+8(FP), R1

	// Load state blocks
	VLD1.P 64(R0), [V0.B16, V1.B16, V2.B16, V3.B16]
	SUB $64, R0, R0

	// Zero register
	VEOR V15.B16, V15.B16, V15.B16

	// Initial round tweakey addition (RTK[0])
	VLD1.P 64(R1), [V4.B16, V5.B16, V6.B16, V7.B16]
	VEOR V4.B16, V0.B16, V0.B16
	VEOR V5.B16, V1.B16, V1.B16
	VEOR V6.B16, V2.B16, V2.B16
	VEOR V7.B16, V3.B16, V3.B16

	// Steps 0-8 with permutation
	MOVD $0, R2

pholkos512_enc_step_loop:
	// Two rounds per step
	VLD1.P 64(R1), [V4.B16, V5.B16, V6.B16, V7.B16]
	AESE V15.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V4.B16, V0.B16, V0.B16
	AESE V15.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V5.B16, V1.B16, V1.B16
	AESE V15.B16, V2.B16
	AESMC V2.B16, V2.B16
	VEOR V6.B16, V2.B16, V2.B16
	AESE V15.B16, V3.B16
	AESMC V3.B16, V3.B16
	VEOR V7.B16, V3.B16, V3.B16

	VLD1.P 64(R1), [V4.B16, V5.B16, V6.B16, V7.B16]
	AESE V15.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V4.B16, V0.B16, V0.B16
	AESE V15.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V5.B16, V1.B16, V1.B16
	AESE V15.B16, V2.B16
	AESMC V2.B16, V2.B16
	VEOR V6.B16, V2.B16, V2.B16
	AESE V15.B16, V3.B16
	AESMC V3.B16, V3.B16
	VEOR V7.B16, V3.B16, V3.B16

	// π512 permutation
	VMOV V0.B16, V8.B16
	VMOV V1.B16, V9.B16
	VMOV V2.B16, V10.B16
	VMOV V3.B16, V11.B16
	VMOV V8.S[0], V0.S[0]
	VMOV V9.S[1], V0.S[1]
	VMOV V10.S[2], V0.S[2]
	VMOV V11.S[3], V0.S[3]
	VMOV V9.S[0], V1.S[0]
	VMOV V10.S[1], V1.S[1]
	VMOV V11.S[2], V1.S[2]
	VMOV V8.S[3], V1.S[3]
	VMOV V10.S[0], V2.S[0]
	VMOV V11.S[1], V2.S[1]
	VMOV V8.S[2], V2.S[2]
	VMOV V9.S[3], V2.S[3]
	VMOV V11.S[0], V3.S[0]
	VMOV V8.S[1], V3.S[1]
	VMOV V9.S[2], V3.S[2]
	VMOV V10.S[3], V3.S[3]

	ADD $1, R2, R2
	CMP $9, R2
	BLT pholkos512_enc_step_loop

	// Step 9 (final): rounds 19-20, no permutation
	VLD1.P 64(R1), [V4.B16, V5.B16, V6.B16, V7.B16]
	AESE V15.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V4.B16, V0.B16, V0.B16
	AESE V15.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V5.B16, V1.B16, V1.B16
	AESE V15.B16, V2.B16
	AESMC V2.B16, V2.B16
	VEOR V6.B16, V2.B16, V2.B16
	AESE V15.B16, V3.B16
	AESMC V3.B16, V3.B16
	VEOR V7.B16, V3.B16, V3.B16

	// Final round (no AESMC)
	VLD1 (R1), [V4.B16, V5.B16, V6.B16, V7.B16]
	AESE V15.B16, V0.B16
	VEOR V4.B16, V0.B16, V0.B16
	AESE V15.B16, V1.B16
	VEOR V5.B16, V1.B16, V1.B16
	AESE V15.B16, V2.B16
	VEOR V6.B16, V2.B16, V2.B16
	AESE V15.B16, V3.B16
	VEOR V7.B16, V3.B16, V3.B16

	// Store result
	MOVD block+0(FP), R0
	VST1 [V0.B16, V1.B16, V2.B16, V3.B16], (R0)
	RET


// Pholkos-512 decryption using ARM Crypto extensions
// func pholkos512DecryptAsm(block *Pholkos512Block, rtk *[21][4]Block)
TEXT ·pholkos512DecryptAsm(SB),NOSPLIT,$0
	MOVD block+0(FP), R0
	MOVD rtk+8(FP), R1

	// Load state blocks
	VLD1 (R0), [V0.B16, V1.B16, V2.B16, V3.B16]

	// Zero register
	VEOR V15.B16, V15.B16, V15.B16

	// Go to RTK[20] (offset = 20 * 64 = 1280)
	ADD $1280, R1, R3

	// Step 9 (first in reverse): undo rounds 20, 19
	// Final round: XOR key, InvSR+InvSB
	VLD1 (R3), [V4.B16, V5.B16, V6.B16, V7.B16]
	VEOR V4.B16, V0.B16, V0.B16
	VEOR V5.B16, V1.B16, V1.B16
	VEOR V6.B16, V2.B16, V2.B16
	VEOR V7.B16, V3.B16, V3.B16
	AESD V15.B16, V0.B16
	AESD V15.B16, V1.B16
	AESD V15.B16, V2.B16
	AESD V15.B16, V3.B16

	// Round 19
	SUB $64, R3, R3
	VLD1 (R3), [V4.B16, V5.B16, V6.B16, V7.B16]
	VEOR V4.B16, V0.B16, V0.B16
	VEOR V5.B16, V1.B16, V1.B16
	VEOR V6.B16, V2.B16, V2.B16
	VEOR V7.B16, V3.B16, V3.B16
	AESIMC V0.B16, V0.B16
	AESIMC V1.B16, V1.B16
	AESIMC V2.B16, V2.B16
	AESIMC V3.B16, V3.B16
	AESD V15.B16, V0.B16
	AESD V15.B16, V1.B16
	AESD V15.B16, V2.B16
	AESD V15.B16, V3.B16

	// Steps 8-0: inverse permutation then two rounds
	MOVD $9, R2

pholkos512_dec_step_loop:
	SUB $1, R2, R2

	// π512 inverse permutation
	VMOV V0.B16, V8.B16
	VMOV V1.B16, V9.B16
	VMOV V2.B16, V10.B16
	VMOV V3.B16, V11.B16
	VMOV V8.S[0], V0.S[0]
	VMOV V11.S[1], V0.S[1]
	VMOV V10.S[2], V0.S[2]
	VMOV V9.S[3], V0.S[3]
	VMOV V9.S[0], V1.S[0]
	VMOV V8.S[1], V1.S[1]
	VMOV V11.S[2], V1.S[2]
	VMOV V10.S[3], V1.S[3]
	VMOV V10.S[0], V2.S[0]
	VMOV V9.S[1], V2.S[1]
	VMOV V8.S[2], V2.S[2]
	VMOV V11.S[3], V2.S[3]
	VMOV V11.S[0], V3.S[0]
	VMOV V10.S[1], V3.S[1]
	VMOV V9.S[2], V3.S[2]
	VMOV V8.S[3], V3.S[3]

	// Two inverse rounds
	SUB $64, R3, R3
	VLD1 (R3), [V4.B16, V5.B16, V6.B16, V7.B16]
	VEOR V4.B16, V0.B16, V0.B16
	VEOR V5.B16, V1.B16, V1.B16
	VEOR V6.B16, V2.B16, V2.B16
	VEOR V7.B16, V3.B16, V3.B16
	AESIMC V0.B16, V0.B16
	AESIMC V1.B16, V1.B16
	AESIMC V2.B16, V2.B16
	AESIMC V3.B16, V3.B16
	AESD V15.B16, V0.B16
	AESD V15.B16, V1.B16
	AESD V15.B16, V2.B16
	AESD V15.B16, V3.B16

	SUB $64, R3, R3
	VLD1 (R3), [V4.B16, V5.B16, V6.B16, V7.B16]
	VEOR V4.B16, V0.B16, V0.B16
	VEOR V5.B16, V1.B16, V1.B16
	VEOR V6.B16, V2.B16, V2.B16
	VEOR V7.B16, V3.B16, V3.B16
	AESIMC V0.B16, V0.B16
	AESIMC V1.B16, V1.B16
	AESIMC V2.B16, V2.B16
	AESIMC V3.B16, V3.B16
	AESD V15.B16, V0.B16
	AESD V15.B16, V1.B16
	AESD V15.B16, V2.B16
	AESD V15.B16, V3.B16

	CBNZ R2, pholkos512_dec_step_loop

	// Initial round key (RTK[0])
	MOVD rtk+8(FP), R1
	VLD1 (R1), [V4.B16, V5.B16, V6.B16, V7.B16]
	VEOR V4.B16, V0.B16, V0.B16
	VEOR V5.B16, V1.B16, V1.B16
	VEOR V6.B16, V2.B16, V2.B16
	VEOR V7.B16, V3.B16, V3.B16

	// Store result
	MOVD block+0(FP), R0
	VST1 [V0.B16, V1.B16, V2.B16, V3.B16], (R0)
	RET
