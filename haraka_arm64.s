// Haraka v2 ARM Crypto hardware acceleration for ARM64
#include "textflag.h"

// func haraka256HW(out *[32]byte, input *[32]byte, rc *[40][16]byte)
TEXT ·haraka256HW(SB),NOSPLIT,$0
	MOVD out+0(FP), R0
	MOVD input+8(FP), R1
	MOVD rc+16(FP), R2

	// Load input blocks into V0 (s0) and V1 (s1)
	VLD1 (R1), [V0.B16, V1.B16]

	// Save originals for feed-forward (V16, V17)
	VMOV V0.B16, V16.B16
	VMOV V1.B16, V17.B16

	// Create zero vector for AESE
	VEOR V31.B16, V31.B16, V31.B16

	// Round 0 (rc[0..3])
	VLD1.P 64(R2), [V2.B16, V3.B16, V4.B16, V5.B16]
	// AES round on s0 with rc[0]
	AESE V31.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V2.B16, V0.B16, V0.B16
	// AES round on s1 with rc[1]
	AESE V31.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V3.B16, V1.B16, V1.B16
	// AES round on s0 with rc[2]
	AESE V31.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V4.B16, V0.B16, V0.B16
	// AES round on s1 with rc[3]
	AESE V31.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V5.B16, V1.B16, V1.B16
	// mix2: ZIP1.S (interleave low 32-bit), ZIP2.S (interleave high 32-bit)
	VMOV V0.B16, V6.B16
	VZIP1 V1.S4, V6.S4, V0.S4   // V0 = [a0, b0, a1, b1]
	VZIP2 V1.S4, V6.S4, V1.S4   // V1 = [a2, b2, a3, b3]

	// Round 1 (rc[4..7])
	VLD1.P 64(R2), [V2.B16, V3.B16, V4.B16, V5.B16]
	AESE V31.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V2.B16, V0.B16, V0.B16
	AESE V31.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V3.B16, V1.B16, V1.B16
	AESE V31.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V4.B16, V0.B16, V0.B16
	AESE V31.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V5.B16, V1.B16, V1.B16
	VMOV V0.B16, V6.B16
	VZIP1 V1.S4, V6.S4, V0.S4
	VZIP2 V1.S4, V6.S4, V1.S4

	// Round 2 (rc[8..11])
	VLD1.P 64(R2), [V2.B16, V3.B16, V4.B16, V5.B16]
	AESE V31.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V2.B16, V0.B16, V0.B16
	AESE V31.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V3.B16, V1.B16, V1.B16
	AESE V31.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V4.B16, V0.B16, V0.B16
	AESE V31.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V5.B16, V1.B16, V1.B16
	VMOV V0.B16, V6.B16
	VZIP1 V1.S4, V6.S4, V0.S4
	VZIP2 V1.S4, V6.S4, V1.S4

	// Round 3 (rc[12..15])
	VLD1.P 64(R2), [V2.B16, V3.B16, V4.B16, V5.B16]
	AESE V31.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V2.B16, V0.B16, V0.B16
	AESE V31.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V3.B16, V1.B16, V1.B16
	AESE V31.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V4.B16, V0.B16, V0.B16
	AESE V31.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V5.B16, V1.B16, V1.B16
	VMOV V0.B16, V6.B16
	VZIP1 V1.S4, V6.S4, V0.S4
	VZIP2 V1.S4, V6.S4, V1.S4

	// Round 4 (rc[16..19])
	VLD1.P 64(R2), [V2.B16, V3.B16, V4.B16, V5.B16]
	AESE V31.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V2.B16, V0.B16, V0.B16
	AESE V31.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V3.B16, V1.B16, V1.B16
	AESE V31.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V4.B16, V0.B16, V0.B16
	AESE V31.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V5.B16, V1.B16, V1.B16
	VMOV V0.B16, V6.B16
	VZIP1 V1.S4, V6.S4, V0.S4
	VZIP2 V1.S4, V6.S4, V1.S4

	// Feed-forward XOR
	VEOR V16.B16, V0.B16, V0.B16
	VEOR V17.B16, V1.B16, V1.B16

	// Store output
	VST1 [V0.B16, V1.B16], (R0)
	RET

// func haraka512HW(out *[32]byte, input *[64]byte, rc *[40][16]byte)
TEXT ·haraka512HW(SB),NOSPLIT,$0
	MOVD out+0(FP), R0
	MOVD input+8(FP), R1
	MOVD rc+16(FP), R2

	// Load input blocks into V0-V3 (s0-s3)
	VLD1.P 32(R1), [V0.B16, V1.B16]
	VLD1 (R1), [V2.B16, V3.B16]

	// Save originals for feed-forward (V16-V19)
	VMOV V0.B16, V16.B16
	VMOV V1.B16, V17.B16
	VMOV V2.B16, V18.B16
	VMOV V3.B16, V19.B16

	// Create zero vector for AESE
	VEOR V31.B16, V31.B16, V31.B16

	// Round 0 (rc[0..7])
	VLD1.P 64(R2), [V4.B16, V5.B16, V6.B16, V7.B16]
	VLD1.P 64(R2), [V8.B16, V9.B16, V10.B16, V11.B16]
	// First AES round on each block
	AESE V31.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V4.B16, V0.B16, V0.B16
	AESE V31.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V5.B16, V1.B16, V1.B16
	AESE V31.B16, V2.B16
	AESMC V2.B16, V2.B16
	VEOR V6.B16, V2.B16, V2.B16
	AESE V31.B16, V3.B16
	AESMC V3.B16, V3.B16
	VEOR V7.B16, V3.B16, V3.B16
	// Second AES round on each block
	AESE V31.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V8.B16, V0.B16, V0.B16
	AESE V31.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V9.B16, V1.B16, V1.B16
	AESE V31.B16, V2.B16
	AESMC V2.B16, V2.B16
	VEOR V10.B16, V2.B16, V2.B16
	AESE V31.B16, V3.B16
	AESMC V3.B16, V3.B16
	VEOR V11.B16, V3.B16, V3.B16
	// mix512: complex permutation
	// new[0]=old[3], new[1]=old[11], new[2]=old[7], new[3]=old[15]
	// new[4]=old[8], new[5]=old[0], new[6]=old[12], new[7]=old[4]
	// new[8]=old[9], new[9]=old[1], new[10]=old[13], new[11]=old[5]
	// new[12]=old[2], new[13]=old[10], new[14]=old[6], new[15]=old[14]
	// Save copies (V20-V23)
	VMOV V0.B16, V20.B16
	VMOV V1.B16, V21.B16
	VMOV V2.B16, V22.B16
	VMOV V3.B16, V23.B16
	// Build using TRN (transpose) and EXT (extract)
	// new V0 = [s0.s[3], s2.s[3], s1.s[3], s3.s[3]]
	VDUP V20.S[3], V4.S4
	VDUP V22.S[3], V5.S4
	VDUP V21.S[3], V6.S4
	VDUP V23.S[3], V7.S4
	VMOV V4.S[0], V0.S[0]
	VMOV V5.S[0], V0.S[1]
	VMOV V6.S[0], V0.S[2]
	VMOV V7.S[0], V0.S[3]
	// new V1 = [s2.s[0], s0.s[0], s3.s[0], s1.s[0]]
	VDUP V22.S[0], V4.S4
	VDUP V20.S[0], V5.S4
	VDUP V23.S[0], V6.S4
	VDUP V21.S[0], V7.S4
	VMOV V4.S[0], V1.S[0]
	VMOV V5.S[0], V1.S[1]
	VMOV V6.S[0], V1.S[2]
	VMOV V7.S[0], V1.S[3]
	// new V2 = [s2.s[1], s0.s[1], s3.s[1], s1.s[1]]
	VDUP V22.S[1], V4.S4
	VDUP V20.S[1], V5.S4
	VDUP V23.S[1], V6.S4
	VDUP V21.S[1], V7.S4
	VMOV V4.S[0], V2.S[0]
	VMOV V5.S[0], V2.S[1]
	VMOV V6.S[0], V2.S[2]
	VMOV V7.S[0], V2.S[3]
	// new V3 = [s0.s[2], s2.s[2], s1.s[2], s3.s[2]]
	VDUP V20.S[2], V4.S4
	VDUP V22.S[2], V5.S4
	VDUP V21.S[2], V6.S4
	VDUP V23.S[2], V7.S4
	VMOV V4.S[0], V3.S[0]
	VMOV V5.S[0], V3.S[1]
	VMOV V6.S[0], V3.S[2]
	VMOV V7.S[0], V3.S[3]

	// Round 1 (rc[8..15])
	VLD1.P 64(R2), [V4.B16, V5.B16, V6.B16, V7.B16]
	VLD1.P 64(R2), [V8.B16, V9.B16, V10.B16, V11.B16]
	AESE V31.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V4.B16, V0.B16, V0.B16
	AESE V31.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V5.B16, V1.B16, V1.B16
	AESE V31.B16, V2.B16
	AESMC V2.B16, V2.B16
	VEOR V6.B16, V2.B16, V2.B16
	AESE V31.B16, V3.B16
	AESMC V3.B16, V3.B16
	VEOR V7.B16, V3.B16, V3.B16
	AESE V31.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V8.B16, V0.B16, V0.B16
	AESE V31.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V9.B16, V1.B16, V1.B16
	AESE V31.B16, V2.B16
	AESMC V2.B16, V2.B16
	VEOR V10.B16, V2.B16, V2.B16
	AESE V31.B16, V3.B16
	AESMC V3.B16, V3.B16
	VEOR V11.B16, V3.B16, V3.B16
	// mix512
	VMOV V0.B16, V20.B16
	VMOV V1.B16, V21.B16
	VMOV V2.B16, V22.B16
	VMOV V3.B16, V23.B16
	VDUP V20.S[3], V4.S4
	VDUP V22.S[3], V5.S4
	VDUP V21.S[3], V6.S4
	VDUP V23.S[3], V7.S4
	VMOV V4.S[0], V0.S[0]
	VMOV V5.S[0], V0.S[1]
	VMOV V6.S[0], V0.S[2]
	VMOV V7.S[0], V0.S[3]
	VDUP V22.S[0], V4.S4
	VDUP V20.S[0], V5.S4
	VDUP V23.S[0], V6.S4
	VDUP V21.S[0], V7.S4
	VMOV V4.S[0], V1.S[0]
	VMOV V5.S[0], V1.S[1]
	VMOV V6.S[0], V1.S[2]
	VMOV V7.S[0], V1.S[3]
	VDUP V22.S[1], V4.S4
	VDUP V20.S[1], V5.S4
	VDUP V23.S[1], V6.S4
	VDUP V21.S[1], V7.S4
	VMOV V4.S[0], V2.S[0]
	VMOV V5.S[0], V2.S[1]
	VMOV V6.S[0], V2.S[2]
	VMOV V7.S[0], V2.S[3]
	VDUP V20.S[2], V4.S4
	VDUP V22.S[2], V5.S4
	VDUP V21.S[2], V6.S4
	VDUP V23.S[2], V7.S4
	VMOV V4.S[0], V3.S[0]
	VMOV V5.S[0], V3.S[1]
	VMOV V6.S[0], V3.S[2]
	VMOV V7.S[0], V3.S[3]

	// Round 2 (rc[16..23])
	VLD1.P 64(R2), [V4.B16, V5.B16, V6.B16, V7.B16]
	VLD1.P 64(R2), [V8.B16, V9.B16, V10.B16, V11.B16]
	AESE V31.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V4.B16, V0.B16, V0.B16
	AESE V31.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V5.B16, V1.B16, V1.B16
	AESE V31.B16, V2.B16
	AESMC V2.B16, V2.B16
	VEOR V6.B16, V2.B16, V2.B16
	AESE V31.B16, V3.B16
	AESMC V3.B16, V3.B16
	VEOR V7.B16, V3.B16, V3.B16
	AESE V31.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V8.B16, V0.B16, V0.B16
	AESE V31.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V9.B16, V1.B16, V1.B16
	AESE V31.B16, V2.B16
	AESMC V2.B16, V2.B16
	VEOR V10.B16, V2.B16, V2.B16
	AESE V31.B16, V3.B16
	AESMC V3.B16, V3.B16
	VEOR V11.B16, V3.B16, V3.B16
	// mix512
	VMOV V0.B16, V20.B16
	VMOV V1.B16, V21.B16
	VMOV V2.B16, V22.B16
	VMOV V3.B16, V23.B16
	VDUP V20.S[3], V4.S4
	VDUP V22.S[3], V5.S4
	VDUP V21.S[3], V6.S4
	VDUP V23.S[3], V7.S4
	VMOV V4.S[0], V0.S[0]
	VMOV V5.S[0], V0.S[1]
	VMOV V6.S[0], V0.S[2]
	VMOV V7.S[0], V0.S[3]
	VDUP V22.S[0], V4.S4
	VDUP V20.S[0], V5.S4
	VDUP V23.S[0], V6.S4
	VDUP V21.S[0], V7.S4
	VMOV V4.S[0], V1.S[0]
	VMOV V5.S[0], V1.S[1]
	VMOV V6.S[0], V1.S[2]
	VMOV V7.S[0], V1.S[3]
	VDUP V22.S[1], V4.S4
	VDUP V20.S[1], V5.S4
	VDUP V23.S[1], V6.S4
	VDUP V21.S[1], V7.S4
	VMOV V4.S[0], V2.S[0]
	VMOV V5.S[0], V2.S[1]
	VMOV V6.S[0], V2.S[2]
	VMOV V7.S[0], V2.S[3]
	VDUP V20.S[2], V4.S4
	VDUP V22.S[2], V5.S4
	VDUP V21.S[2], V6.S4
	VDUP V23.S[2], V7.S4
	VMOV V4.S[0], V3.S[0]
	VMOV V5.S[0], V3.S[1]
	VMOV V6.S[0], V3.S[2]
	VMOV V7.S[0], V3.S[3]

	// Round 3 (rc[24..31])
	VLD1.P 64(R2), [V4.B16, V5.B16, V6.B16, V7.B16]
	VLD1.P 64(R2), [V8.B16, V9.B16, V10.B16, V11.B16]
	AESE V31.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V4.B16, V0.B16, V0.B16
	AESE V31.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V5.B16, V1.B16, V1.B16
	AESE V31.B16, V2.B16
	AESMC V2.B16, V2.B16
	VEOR V6.B16, V2.B16, V2.B16
	AESE V31.B16, V3.B16
	AESMC V3.B16, V3.B16
	VEOR V7.B16, V3.B16, V3.B16
	AESE V31.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V8.B16, V0.B16, V0.B16
	AESE V31.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V9.B16, V1.B16, V1.B16
	AESE V31.B16, V2.B16
	AESMC V2.B16, V2.B16
	VEOR V10.B16, V2.B16, V2.B16
	AESE V31.B16, V3.B16
	AESMC V3.B16, V3.B16
	VEOR V11.B16, V3.B16, V3.B16
	// mix512
	VMOV V0.B16, V20.B16
	VMOV V1.B16, V21.B16
	VMOV V2.B16, V22.B16
	VMOV V3.B16, V23.B16
	VDUP V20.S[3], V4.S4
	VDUP V22.S[3], V5.S4
	VDUP V21.S[3], V6.S4
	VDUP V23.S[3], V7.S4
	VMOV V4.S[0], V0.S[0]
	VMOV V5.S[0], V0.S[1]
	VMOV V6.S[0], V0.S[2]
	VMOV V7.S[0], V0.S[3]
	VDUP V22.S[0], V4.S4
	VDUP V20.S[0], V5.S4
	VDUP V23.S[0], V6.S4
	VDUP V21.S[0], V7.S4
	VMOV V4.S[0], V1.S[0]
	VMOV V5.S[0], V1.S[1]
	VMOV V6.S[0], V1.S[2]
	VMOV V7.S[0], V1.S[3]
	VDUP V22.S[1], V4.S4
	VDUP V20.S[1], V5.S4
	VDUP V23.S[1], V6.S4
	VDUP V21.S[1], V7.S4
	VMOV V4.S[0], V2.S[0]
	VMOV V5.S[0], V2.S[1]
	VMOV V6.S[0], V2.S[2]
	VMOV V7.S[0], V2.S[3]
	VDUP V20.S[2], V4.S4
	VDUP V22.S[2], V5.S4
	VDUP V21.S[2], V6.S4
	VDUP V23.S[2], V7.S4
	VMOV V4.S[0], V3.S[0]
	VMOV V5.S[0], V3.S[1]
	VMOV V6.S[0], V3.S[2]
	VMOV V7.S[0], V3.S[3]

	// Round 4 (rc[32..39])
	VLD1.P 64(R2), [V4.B16, V5.B16, V6.B16, V7.B16]
	VLD1 (R2), [V8.B16, V9.B16, V10.B16, V11.B16]
	AESE V31.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V4.B16, V0.B16, V0.B16
	AESE V31.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V5.B16, V1.B16, V1.B16
	AESE V31.B16, V2.B16
	AESMC V2.B16, V2.B16
	VEOR V6.B16, V2.B16, V2.B16
	AESE V31.B16, V3.B16
	AESMC V3.B16, V3.B16
	VEOR V7.B16, V3.B16, V3.B16
	AESE V31.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V8.B16, V0.B16, V0.B16
	AESE V31.B16, V1.B16
	AESMC V1.B16, V1.B16
	VEOR V9.B16, V1.B16, V1.B16
	AESE V31.B16, V2.B16
	AESMC V2.B16, V2.B16
	VEOR V10.B16, V2.B16, V2.B16
	AESE V31.B16, V3.B16
	AESMC V3.B16, V3.B16
	VEOR V11.B16, V3.B16, V3.B16
	// mix512
	VMOV V0.B16, V20.B16
	VMOV V1.B16, V21.B16
	VMOV V2.B16, V22.B16
	VMOV V3.B16, V23.B16
	VDUP V20.S[3], V4.S4
	VDUP V22.S[3], V5.S4
	VDUP V21.S[3], V6.S4
	VDUP V23.S[3], V7.S4
	VMOV V4.S[0], V0.S[0]
	VMOV V5.S[0], V0.S[1]
	VMOV V6.S[0], V0.S[2]
	VMOV V7.S[0], V0.S[3]
	VDUP V22.S[0], V4.S4
	VDUP V20.S[0], V5.S4
	VDUP V23.S[0], V6.S4
	VDUP V21.S[0], V7.S4
	VMOV V4.S[0], V1.S[0]
	VMOV V5.S[0], V1.S[1]
	VMOV V6.S[0], V1.S[2]
	VMOV V7.S[0], V1.S[3]
	VDUP V22.S[1], V4.S4
	VDUP V20.S[1], V5.S4
	VDUP V23.S[1], V6.S4
	VDUP V21.S[1], V7.S4
	VMOV V4.S[0], V2.S[0]
	VMOV V5.S[0], V2.S[1]
	VMOV V6.S[0], V2.S[2]
	VMOV V7.S[0], V2.S[3]
	VDUP V20.S[2], V4.S4
	VDUP V22.S[2], V5.S4
	VDUP V21.S[2], V6.S4
	VDUP V23.S[2], V7.S4
	VMOV V4.S[0], V3.S[0]
	VMOV V5.S[0], V3.S[1]
	VMOV V6.S[0], V3.S[2]
	VMOV V7.S[0], V3.S[3]

	// Feed-forward XOR
	VEOR V16.B16, V0.B16, V0.B16
	VEOR V17.B16, V1.B16, V1.B16
	VEOR V18.B16, V2.B16, V2.B16
	VEOR V19.B16, V3.B16, V3.B16

	// Truncated output: s0[8:16] || s1[8:16] || s2[0:8] || s3[0:8]
	// Extract high qword from V0 and V1, low qword from V2 and V3
	VEXT $8, V0.B16, V0.B16, V4.B16    // V4 = high 8 bytes of V0
	VEXT $8, V1.B16, V1.B16, V5.B16    // V5 = high 8 bytes of V1
	// Combine: V4 low || V5 low -> first 16 bytes out
	VMOV V4.D[0], V6.D[0]
	VMOV V5.D[0], V6.D[1]
	// V2 low || V3 low -> second 16 bytes out
	VMOV V2.D[0], V7.D[0]
	VMOV V3.D[0], V7.D[1]
	VST1 [V6.B16, V7.B16], (R0)
	RET
