// Multi-round AES operations using ARM Crypto extensions
// Optimized to keep block in register across multiple rounds
#include "textflag.h"

// ARM AESE does: XOR key first, THEN ShiftRows + SubBytes
// To match standard AES round (Intel semantics), we use zero key with AESE
// then XOR the actual key at the end of each round

// func armRounds4(block *Block, roundKeys *RoundKeys4)
TEXT ·armRounds4(SB),NOSPLIT,$0
	MOVD block+0(FP), R0
	MOVD roundKeys+8(FP), R1

	// Load block into V0
	VLD1 (R0), [V0.B16]

	// Create zero vector for AESE
	VEOR V2.B16, V2.B16, V2.B16

	// Round 1
	VLD1 (R1), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 2
	ADD $16, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 3
	ADD $32, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 4
	ADD $48, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Store result
	VST1 [V0.B16], (R0)
	RET

// func armInvRounds4(block *Block, roundKeys *RoundKeys4)
TEXT ·armInvRounds4(SB),NOSPLIT,$0
	MOVD block+0(FP), R0
	MOVD roundKeys+8(FP), R1

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	// Round 1
	VLD1 (R1), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 2
	ADD $16, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 3
	ADD $32, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 4
	ADD $48, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// func armRounds6(block *Block, roundKeys *RoundKeys6)
TEXT ·armRounds6(SB),NOSPLIT,$0
	MOVD block+0(FP), R0
	MOVD roundKeys+8(FP), R1

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	// Round 1
	VLD1 (R1), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 2
	ADD $16, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 3
	ADD $32, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 4
	ADD $48, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 5
	ADD $64, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 6
	ADD $80, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// func armInvRounds6(block *Block, roundKeys *RoundKeys6)
TEXT ·armInvRounds6(SB),NOSPLIT,$0
	MOVD block+0(FP), R0
	MOVD roundKeys+8(FP), R1

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	VLD1 (R1), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $16, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $32, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $48, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $64, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $80, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// func armRounds7(block *Block, roundKeys *RoundKeys7)
TEXT ·armRounds7(SB),NOSPLIT,$0
	MOVD block+0(FP), R0
	MOVD roundKeys+8(FP), R1

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	// Round 1
	VLD1 (R1), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 2
	ADD $16, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 3
	ADD $32, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 4
	ADD $48, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 5
	ADD $64, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 6
	ADD $80, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 7
	ADD $96, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// func armInvRounds7(block *Block, roundKeys *RoundKeys7)
TEXT ·armInvRounds7(SB),NOSPLIT,$0
	MOVD block+0(FP), R0
	MOVD roundKeys+8(FP), R1

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	VLD1 (R1), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $16, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $32, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $48, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $64, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $80, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $96, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// func armRounds10(block *Block, roundKeys *RoundKeys10)
TEXT ·armRounds10(SB),NOSPLIT,$0
	MOVD block+0(FP), R0
	MOVD roundKeys+8(FP), R1

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	VLD1 (R1), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $16, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $32, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $48, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $64, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $80, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $96, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $112, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $128, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $144, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// func armInvRounds10(block *Block, roundKeys *RoundKeys10)
TEXT ·armInvRounds10(SB),NOSPLIT,$0
	MOVD block+0(FP), R0
	MOVD roundKeys+8(FP), R1

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	VLD1 (R1), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $16, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $32, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $48, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $64, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $80, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $96, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $112, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $128, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $144, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// func armRounds12(block *Block, roundKeys *RoundKeys12)
TEXT ·armRounds12(SB),NOSPLIT,$0
	MOVD block+0(FP), R0
	MOVD roundKeys+8(FP), R1

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	VLD1 (R1), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $16, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $32, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $48, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $64, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $80, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $96, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $112, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $128, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $144, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $160, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $176, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// func armInvRounds12(block *Block, roundKeys *RoundKeys12)
TEXT ·armInvRounds12(SB),NOSPLIT,$0
	MOVD block+0(FP), R0
	MOVD roundKeys+8(FP), R1

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	VLD1 (R1), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $16, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $32, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $48, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $64, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $80, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $96, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $112, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $128, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $144, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $160, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $176, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// func armRounds14(block *Block, roundKeys *RoundKeys14)
TEXT ·armRounds14(SB),NOSPLIT,$0
	MOVD block+0(FP), R0
	MOVD roundKeys+8(FP), R1

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	VLD1 (R1), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $16, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $32, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $48, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $64, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $80, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $96, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $112, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $128, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $144, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $160, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $176, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $192, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $208, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// func armInvRounds14(block *Block, roundKeys *RoundKeys14)
TEXT ·armInvRounds14(SB),NOSPLIT,$0
	MOVD block+0(FP), R0
	MOVD roundKeys+8(FP), R1

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	VLD1 (R1), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $16, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $32, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $48, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $64, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $80, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $96, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $112, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $128, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $144, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $160, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $176, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $192, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	ADD $208, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// NoKey variants - perform rounds without AddRoundKey
// These use zero key with AESE/AESD, which means no XOR at the end

// func armRounds4NoKey(block *Block)
TEXT ·armRounds4NoKey(SB),NOSPLIT,$0
	MOVD block+0(FP), R0

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// func armInvRounds4NoKey(block *Block)
TEXT ·armInvRounds4NoKey(SB),NOSPLIT,$0
	MOVD block+0(FP), R0

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// func armRounds7NoKey(block *Block)
TEXT ·armRounds7NoKey(SB),NOSPLIT,$0
	MOVD block+0(FP), R0

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// func armInvRounds7NoKey(block *Block)
TEXT ·armInvRounds7NoKey(SB),NOSPLIT,$0
	MOVD block+0(FP), R0

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// func armRounds10NoKey(block *Block)
TEXT ·armRounds10NoKey(SB),NOSPLIT,$0
	MOVD block+0(FP), R0

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// func armInvRounds10NoKey(block *Block)
TEXT ·armInvRounds10NoKey(SB),NOSPLIT,$0
	MOVD block+0(FP), R0

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// func armRounds12NoKey(block *Block)
TEXT ·armRounds12NoKey(SB),NOSPLIT,$0
	MOVD block+0(FP), R0

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// func armInvRounds12NoKey(block *Block)
TEXT ·armInvRounds12NoKey(SB),NOSPLIT,$0
	MOVD block+0(FP), R0

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// func armRounds14NoKey(block *Block)
TEXT ·armRounds14NoKey(SB),NOSPLIT,$0
	MOVD block+0(FP), R0

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// func armInvRounds14NoKey(block *Block)
TEXT ·armInvRounds14NoKey(SB),NOSPLIT,$0
	MOVD block+0(FP), R0

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// WithFinal variants: N-1 full rounds + 1 final round (without MixColumns)

// func armRounds6WithFinal(block *Block, roundKeys *RoundKeys6)
// 5 full rounds + 1 final round
TEXT ·armRounds6WithFinal(SB),NOSPLIT,$0
	MOVD block+0(FP), R0
	MOVD roundKeys+8(FP), R1

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	// Round 1 (full)
	VLD1 (R1), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 2 (full)
	ADD $16, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 3 (full)
	ADD $32, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 4 (full)
	ADD $48, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 5 (full)
	ADD $64, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 6 (final - no MixColumns)
	ADD $80, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// func armRounds10WithFinal(block *Block, roundKeys *RoundKeys10)
// 9 full rounds + 1 final round
TEXT ·armRounds10WithFinal(SB),NOSPLIT,$0
	MOVD block+0(FP), R0
	MOVD roundKeys+8(FP), R1

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	// Round 1
	VLD1 (R1), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 2
	ADD $16, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 3
	ADD $32, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 4
	ADD $48, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 5
	ADD $64, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 6
	ADD $80, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 7
	ADD $96, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 8
	ADD $112, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 9
	ADD $128, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 10 (final - no MixColumns)
	ADD $144, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// func armRounds12WithFinal(block *Block, roundKeys *RoundKeys12)
// 11 full rounds + 1 final round
TEXT ·armRounds12WithFinal(SB),NOSPLIT,$0
	MOVD block+0(FP), R0
	MOVD roundKeys+8(FP), R1

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	// Round 1
	VLD1 (R1), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 2
	ADD $16, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 3
	ADD $32, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 4
	ADD $48, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 5
	ADD $64, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 6
	ADD $80, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 7
	ADD $96, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 8
	ADD $112, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 9
	ADD $128, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 10
	ADD $144, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 11
	ADD $160, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 12 (final - no MixColumns)
	ADD $176, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// func armRounds14WithFinal(block *Block, roundKeys *RoundKeys14)
// 13 full rounds + 1 final round
TEXT ·armRounds14WithFinal(SB),NOSPLIT,$0
	MOVD block+0(FP), R0
	MOVD roundKeys+8(FP), R1

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	// Round 1
	VLD1 (R1), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 2
	ADD $16, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 3
	ADD $32, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 4
	ADD $48, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 5
	ADD $64, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 6
	ADD $80, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 7
	ADD $96, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 8
	ADD $112, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 9
	ADD $128, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 10
	ADD $144, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 11
	ADD $160, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 12
	ADD $176, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 13
	ADD $192, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	AESMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 14 (final - no MixColumns)
	ADD $208, R1, R3
	VLD1 (R3), [V1.B16]
	AESE V2.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// InvWithFinal variants: N-1 full decryption rounds + 1 final decryption round (without InvMixColumns)

// func armInvRounds4WithFinal(block *Block, roundKeys *RoundKeys4)
// 3 full rounds + 1 final round
TEXT ·armInvRounds4WithFinal(SB),NOSPLIT,$0
	MOVD block+0(FP), R0
	MOVD roundKeys+8(FP), R1

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	// Round 1 (full)
	VLD1 (R1), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 2 (full)
	ADD $16, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 3 (full)
	ADD $32, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 4 (final - no InvMixColumns)
	ADD $48, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// func armInvRounds6WithFinal(block *Block, roundKeys *RoundKeys6)
// 5 full rounds + 1 final round
TEXT ·armInvRounds6WithFinal(SB),NOSPLIT,$0
	MOVD block+0(FP), R0
	MOVD roundKeys+8(FP), R1

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	// Round 1 (full)
	VLD1 (R1), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 2 (full)
	ADD $16, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 3 (full)
	ADD $32, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 4 (full)
	ADD $48, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 5 (full)
	ADD $64, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 6 (final - no InvMixColumns)
	ADD $80, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// func armInvRounds7WithFinal(block *Block, roundKeys *RoundKeys7)
// 6 full rounds + 1 final round
TEXT ·armInvRounds7WithFinal(SB),NOSPLIT,$0
	MOVD block+0(FP), R0
	MOVD roundKeys+8(FP), R1

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	// Round 1
	VLD1 (R1), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 2
	ADD $16, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 3
	ADD $32, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 4
	ADD $48, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 5
	ADD $64, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 6
	ADD $80, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 7 (final - no InvMixColumns)
	ADD $96, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// func armInvRounds10WithFinal(block *Block, roundKeys *RoundKeys10)
// 9 full rounds + 1 final round
TEXT ·armInvRounds10WithFinal(SB),NOSPLIT,$0
	MOVD block+0(FP), R0
	MOVD roundKeys+8(FP), R1

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	// Round 1
	VLD1 (R1), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 2
	ADD $16, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 3
	ADD $32, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 4
	ADD $48, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 5
	ADD $64, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 6
	ADD $80, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 7
	ADD $96, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 8
	ADD $112, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 9
	ADD $128, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 10 (final - no InvMixColumns)
	ADD $144, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// func armInvRounds12WithFinal(block *Block, roundKeys *RoundKeys12)
// 11 full rounds + 1 final round
TEXT ·armInvRounds12WithFinal(SB),NOSPLIT,$0
	MOVD block+0(FP), R0
	MOVD roundKeys+8(FP), R1

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	// Round 1
	VLD1 (R1), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 2
	ADD $16, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 3
	ADD $32, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 4
	ADD $48, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 5
	ADD $64, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 6
	ADD $80, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 7
	ADD $96, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 8
	ADD $112, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 9
	ADD $128, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 10
	ADD $144, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 11
	ADD $160, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 12 (final - no InvMixColumns)
	ADD $176, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET

// func armInvRounds14WithFinal(block *Block, roundKeys *RoundKeys14)
// 13 full rounds + 1 final round
TEXT ·armInvRounds14WithFinal(SB),NOSPLIT,$0
	MOVD block+0(FP), R0
	MOVD roundKeys+8(FP), R1

	VLD1 (R0), [V0.B16]
	VEOR V2.B16, V2.B16, V2.B16

	// Round 1
	VLD1 (R1), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 2
	ADD $16, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 3
	ADD $32, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 4
	ADD $48, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 5
	ADD $64, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 6
	ADD $80, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 7
	ADD $96, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 8
	ADD $112, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 9
	ADD $128, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 10
	ADD $144, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 11
	ADD $160, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 12
	ADD $176, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 13
	ADD $192, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	AESIMC V0.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	// Round 14 (final - no InvMixColumns)
	ADD $208, R1, R3
	VLD1 (R3), [V1.B16]
	AESD V2.B16, V0.B16
	VEOR V1.B16, V0.B16, V0.B16

	VST1 [V0.B16], (R0)
	RET
